
\section{Adaptive Proposal Distribution}

\paragraph{}I use an adaptive proposal distribution $q_\theta(\theta)$ to sample the parameter space $\theta$. The adaptive metropolis hasting algorithm provides systematic method for modifying the shape of the proposal distribution based on the accepted steps of the current markov chain, allowing for more efficient mixing of chains. That is the $q_\theta(\theta_i) = N(\theta_i, \Sigma_i(\theta_i))$ follows a Gaussian distribution. To provide a reasonable estimate for the covariance matrix $\Sigma_i$, the Markov chain runs for an initial number of steps ($T_{init}$) from a truncated multivariate normal proposal distribution with a covariance matrix, $I_s$, whose entries are calculated using the upper and lower bounds of the support of the priors $[s^k_0, s^k_1] \in \mathcal{S}$, through $i_{k,k} = (s^k_1 - s^k_0)/\zeta$ and $i_{i,j} = 0$ otherwise, where $\zeta$ is a scaling factor.
 
Problematically, the proposal distribution using the updated covariance matrix, $\Sigma_i$, is no longer memoryless, and therefore chain may no longer converge to the correct stationary distribution. To overcome this problem, the proposal distribution must also sample from a non-adaptive multivariate Gaussian distribution modified to ensure that changes to the covariance matrix diminish over time. Further, to improve chain mixing and to optimise convergence rates, I include adaptive scaling factors, $\lambda_i$ and $M_i$ for the initial non-adaptive and adaptive proposals, respectively, whose magnitude diminishes with the number of steps in the chain. The adaptive scaling factor for the non-adaptive proposal distributions stops once the model starts sampling from the adaptive proposal distributions. Overall, the combined non-adaptive and adaptive proposal distributions for the adaptive Metropolis Hastings is given by

\begin{equation}
\begin{array}{l | l | l}
\label{eq:proposal}

i & i \leq T_{init} & i > T_{init} \\ \hline
q(\cdot|\theta_i)&\mathcal{N}(\theta_i, \exp(\lambda_i)I_{s};\mathcal{S})  &


\begin{array}{ll}
\mathcal{N}(\theta_i, \Sigma_i;\mathcal{S}) & \text{with probability }\beta, \\
\mathcal{N}(\theta_i, \exp(\lambda_{t_{init}})I_{s};\mathcal{S}) & \text{with probability }1 - \beta \\ 
\end{array}
 \end{array}
 \end{equation}
 
 
where $\Sigma_t = \exp(M_i)\Gamma_i$ and $M_{i}$, $\lambda_i$ and $\Gamma_{i}$ are updated iteratively through the stochastic approximation algorithm:
 \begin{center}
 \begin{tabular}{l l l}
  $\lambda_{i+1}$ & = & $ \lambda_i + \gamma_1(i)(a(\theta_i, \theta^*) - 0.234) $\\
 $M_{i+1}$& = & $M_i + \gamma_2(i)(a(\theta_t, \theta^*)- 0.234) $ \\
 $\mu_{i+1}$ & = & $\mu_i + \gamma_3(i)(\mu_t - \theta_t) $ \\
 $\Gamma_{i+1}$ & = & $ \Gamma_i + \gamma_4(i)[(\theta_i - \mu_{i+1}))(\theta_t - \mu_{i+1})^T - \Gamma_t]$ \\
\end{tabular} 
 \end{center}
 
 where $ \gamma_i(t)$ are gain factors. Note when $i > T_{init}$ up stop updaing $\lambda_i$.
 
 
\paragraph{}In our implementation, we define $\theta_{i, adapt} = \{M_i, \mu_i, \Gamma_i, \lambda_i\}$, and choose values, $\beta = 0.05$, $\zeta = 100$,  $\lambda_0 = \log(0.1^2/|\theta_i|)$, $M_0 = \log(2.382^2/|\theta_i|)$,  $\mu_0 = \pi_0$,  $\Gamma_0 = I_s$, and $\gamma_x(i) = (1 + i)^{-0.5}$ for all $x$. 

